# Super Resolution for Medical Images 

&nbsp;

## Project Description ‚≠ê  
The application of 'Deep Learning' in Computer Vision has always fascinated me. Among the many real-world applications of deep learning, 'Image Inpainting' stands out, as it involves filling in missing or corrupted parts of an image using information from nearby areas. One of the most crucial applications closely related to image inpainting is 'Super-Resolution', which is the process of recovering and reconstructing the resolution of a noisy low-quality image into a high-quality, high-resolution image. Super-resolution can be applied to medical imaging such as 'MRI', 'CT Scans', and 'X-rays' to recover missing or degraded details, thereby supporting more accurate diagnosis.

'Medical Imaging' often involves capturing images using equipment that has limited spatial resolution, such as X-ray machines or MRI scanners. This can result in images that are pixelated and blurry, making it difficult for doctors to identify subtle abnormalities. Moreover, to capture very high-resolution X-ray images, the number of pixels recorded by the detector must be increased. However, this alone is not sufficient to obtain high-quality images. In practice, higher-resolution X-ray acquisition also requires increased radiation exposure, which can be harmful to patients. Typical X-ray imaging systems capture images at resolutions such as '1024 √ó 1024', and some advanced systems capture up to '3072 √ó 3072', but this comes at the cost of higher radiation dosage.

Instead of exposing patients to increased 'Radiation', a safer alternative is to acquire low-resolution X-ray images (approximately '256 √ó 256') and apply 'Image Super-Resolution' techniques to enhance their quality. By reconstructing missing or degraded high-frequency details, deep learning models can generate sharper and more informative images without additional radiation exposure. This approach has the potential to significantly improve diagnostic accuracy while maintaining patient safety.

There are several state-of-the-art deep learning architectures that have been developed for image super-resolution. In this project, I experimented with multiple super-resolution models, including 'SRCNN', 'VDSR', 'Residual Dense Networks (RDN)', and 'DenseNet-based architectures' that leverage densely connected feature reuse. These models were analyzed to understand how residual learning, dense connectivity, and network depth contribute to effective reconstruction of fine-grained anatomical details in medical images.

In addition to evaluating these architectures individually, I applied 'Knowledge Distillation' techniques to transfer knowledge from larger, high-capacity teacher models to lightweight student models. Furthermore, I experimented with 'Width and Depth Compression' strategies within the distillation framework to reduce computational complexity and model size while preserving reconstruction performance. This enables the deployment of efficient super-resolution models in resource-constrained clinical environments.

The 'Aim' of this project is to systematically study and understand the behavior of these super-resolution architectures and compression techniques on a dataset of 'Chest X-ray' images. High-quality images are downscaled to generate low-resolution inputs ('256 √ó 256'), and the models are trained to reconstruct high-resolution outputs ('1024 √ó 1024'). Model performance is evaluated using standard image quality metrics such as 'PSNR (Peak Signal-to-Noise Ratio)' and 'SSIM (Structural Similarity Index)'.

&nbsp;
## Data Sourcing & Processing üíæ  
For this project the Dataset used to train the Super Resolution model is [NIH Chest X-ray](https://www.kaggle.com/datasets/nih-chest-xrays/data). This NIH Chest X-ray Dataset is comprised of 112,120 high resolution X-ray images with disease labels from 30,805 unique patients. Even though this dataset is primally for lung disease identification, I striped off the labels for each image and instead use these high-resolution X-ray images for training my super-resolution GAN.

The original images have a high resolution of 1024 x 1024. To prepare this dataset for training a super resolution GAN, I downsampled the orignal high resolution images to 256 x 256 (one fourth) using BICUBIC interpolation from the PIL module. The downsampled images are served as an input to the generator architecture which then tries to generate a super resolution image which is as close as possible to the original higher resolution images. The data preprocessing script `scripts/prepare_data.py` is a part of the custom Train and Val data loader classes and is run automatically during the model training part. The data can be donwloaded using a script `scripts/make_dataset.py` and split into train and validation datasets using `scripts/split_dataset.py`. The scripts can be run as follows:

<!-- **1. Create a new conda environment and activate it:** 
```
conda create --name image_super_res python=3.9.16
conda activate image_super_res
```
**2. Install python package requirements:** 
```
pip install -r requirements.txt 
```
**3. Run the data downloading script:** 
```
python ./scripts/make_dataset.py
```
Running this script would prompt you to type your kaggle username and token key (from profile settings) in the terminal. Following that the data would be donwloaded and available in the `./data/` directory.

**4. Split the dataset into train and validation:** 
```
python ./scripts/split_dataset.py
```
This would create two files in the `./data/` directory called `train_images.pkl` and `val_images.pkl` which would store the paths to train and validation split of images   -->

&nbsp;  
## Naive Non Deep Learning Approach ‚ú®    
Non deep learning approaches for image generation may not be as effective as GANs for several reasons such as:
- Non DL approaches have limited representation power compared to deep learning approaches like GANs as Neural networks can learn and generate complex features
- Non DL approaches may not be able to learn from large datasets as effectively as deep learning models
- Non DL approaches may not be able to generalize well to new or unseen data    
Overall, while non deep learning approaches can be useful for certain image generation tasks, they may not be as effective or versatile as GANs, which have proven to be a powerful tool for generating high-quality and diverse images.  
  
&nbsp;  
To test my hypothesis before training a GAN, I used a naive method of `BICUBIC Interpolation` to generate and recover high resolution images from low resolution images. Bicubic image interpolation is a method of increasing the size of an image using mathematical algorithms to estimate the values of additional pixels that are inserted into the image. A cubic polynomial is used to interpolate the pixel values in both the horizontal and vertical directions. It works by calculating the pixel values of the new, enlarged image based on a weighted average of surrounding pixels in the original image.  

<!-- &nbsp;  
### Following are the steps to run this naive approach:  
**1. Activate conda environment:** 
```
conda activate image_super_res
```
**2. Run the python script** 
- You can generate results for naive approach using driver python script : `scripts/non_dl_super_resolution.py`
```
python ./scripts/non_dl_super_resolution.py
```  
The metrics results are saved a csv to the `./logs/` folder with the filename `non_dl_approach_metrics.csv`   -->




&nbsp;
## Model Training and Evaluation üöÇ  

The GAN model was evaluated and compared against ground truths using different metrics like Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index (SSIM). 
- The PSNR is calculated by comparing the original signal to the processed signal, and it is expressed in decibels (dB). The higher the PSNR value, the less distortion or loss of information there is in the processed signal compared to the original signal.
- Similarly, SSIM lies between -1 and 1 and a higher SSIM score indicates a higher similarity between the two images structurally. 
- Compared to PSNR, SSIM is often considered a more perceptually accurate metric, as it takes into account the human visual system's sensitivity to changes in luminance, contrast, and structure.

The model was trained on RTX6000 with a batch size of 16. Following are the metrics obtained after training the models on full dataset for 10 epochs:  

            
| Metric                              |       10 Epochs (DL)      |       1 Epochs (DL)      |      Bicubic (Non DL)    |  
| ----------------------------------- | :-----------------------: | :----------------------: | :----------------------: | 
| Peak Signal to Noise Ratio (PSNR)   |         41.66 (dB)        |         30.37 (dB)       |         30.40 (dB)       |
| Structural Similarity Index (SSIM)  |            0.96           |            0.83          |            0.74          |    
  

<!-- &nbsp;
### Following are the steps to run the model training code:

**1. Activate conda environment:** 
```
conda activate image_super_res
```
**2. To train the model using python script** 
- You can train a model direcltly by runnning the driver python script : `scripts/train_model.py`
- You can pass `batch_size`, `num_epochs`, `upscale_factor` as arguments
- You will need a GPU to train the model
```
python ./scripts/train_model.py  --upscale_factor 4 --num_epochs 10 --batch_size 16
```
**5. Model checkpoints and results** 
- The trained genertor and Discriminator are saved to `./models/` directory after every epoch. The save format is `netG_{UPSCALE_FACTOR}x_epoch{epoch}.pth.tar`
- The metrics results are saved a csv to the `./logs/` folder with the filename `metrics_{epoch}_train_results.csv`   -->
  
&nbsp;
## Risks and Limitations ‚ö†Ô∏è  
1. Generative networks may struggle to accurately capture important details in extremely low resolution medical X-ray images (< 128 x128), which could negatively impact the generated high quality images. 
2. The network may generate features that dont exist in the original low resolution images.
3. The use of generative networks in medical imaging raises ethical concerns around issues such as bias, accountability, and transparency.  

To minimize the risk of the above mentioned biases, the model was trained on a diverse dataset of X-ray images. Furthermore, addtion of perceptual loss to the model helps to ensure that the generated images are similar to the original images and no new features are generated while generating the super resolution images.  

&nbsp;
## Custom Loss Function üéØ  
I have used the same loss function mentioned by the authors of the Swift-SRGAN or SRGAN paper. The loss function for the Generator is a combination of multiple losses, each weighted and added together. The most crucial loss is the Perceptual Loss which is a combination of Adversarial Loss and Content Loss.
&nbsp;  
```
Total_Loss = Image_Loss + Perception_Loss + TV_Loss
```
```
Perceptual_Loss = Adversarial_Loss + Content_Loss
```  

&nbsp;  
**_Loss 1: Image Loss_**  
This is a naive loss functionn whihc calculates the Mean Squared Error b/w the generated image and the original high res image pixels.  
  
&nbsp;  
**_Loss 2: Content Loss_**  
It represents the information that is lost or distorted during the processing of an image. The image generated by the generator and the original high res image are passed though the MobileNetV2 network to compute the feature vectors of both the images. Content loss is calculated as the euclidean distance b/w the feature vectors of the original image and the generated image.  
  

  
&nbsp;  
**_Loss 3:  Total Variation loss_**  
It measures the variation or changes in intensity or color between adjacent pixels in an image. It is defined as the sum of the absolute differences between neighboring pixels in both the horizontal and vertical directions in an image.  
  



```


```
## Project Structure üß¨  
The project data and codes are arranged in the following manner:

```
‚îú‚îÄ‚îÄ data/                                # Directory containing project data  
‚îÇ   ‚îú‚îÄ‚îÄ Normal_images/                   # List of image paths for normal lung images  
‚îÇ   ‚îî‚îÄ‚îÄ Pneumonia_Images/                # List of image paths for lungs with Pneumonia  
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                           # Jupyter notebooks for experiments  
‚îÇ   ‚îú‚îÄ‚îÄ Mobilenet/                       # Notebooks using MobileNet as backbone  
‚îÇ   ‚îú‚îÄ‚îÄ VGG19/                           # Notebooks using VGG19 as backbone  
‚îÇ   ‚îî‚îÄ‚îÄ VGG16/                           # Notebooks using VGG16 as backbone  
‚îÇ
‚îú‚îÄ‚îÄ .gitignore                           # Git ignore file  
‚îú‚îÄ‚îÄ README.md                            # Project description and setup guide  
‚îú‚îÄ‚îÄ requirements.txt                     # Python dependencies file  
```



&nbsp;  
## References üìö   
1. NIH Chest X-rays Dataset from Kaggle [(link)](https://www.kaggle.com/datasets/nih-chest-xrays/data)  

2. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, May 2017, Christian Ledig et al. [(link)](https://arxiv.org/pdf/1609.04802.pdf)  

3. SwiftSRGAN - Rethinking Super-Resolution for Efficient and Real-time Inference, Nov 2021, Koushik Sivarama Krishnan et al. [(link)](https://arxiv.org/pdf/2111.14320.pdf)  

4. SRGAN Loss explanation [(link)](https://towardsdatascience.com/srgan-a-tensorflow-implementation-49b959267c60)  

5. Tensorflow implementation of SRGRAN [(link)](https://github.com/brade31919/SRGAN-tensorflow)



